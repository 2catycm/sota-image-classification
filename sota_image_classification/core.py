# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['partial_vtab', 'get_vtab_dataset', 'ClassificationTaskConfig', 'init_env', 'init_data', 'ClassificationTask']

# %% ../nbs/00_core.ipynb 3
from pydantic import BaseModel
class ClassificationTaskConfig(BaseModel):
    vtab_dir:str = "/home/ai_pitch_perfector/datasets/vtab-1k/"
    subset_name:str = "cifar"
    initial_batch_size:int = 64
    experiment_index:int = 0 # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 表示是第几次重复实验

# %% ../nbs/00_core.ipynb 7
import lightning as L
def init_env(seed:int=42):
    # Ensure that all operations are deterministic on GPU (if used) for reproducibility
    L.seed_everything(seed)
    import torch
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

from . import data
from .data.vtab import VtabSplit, VtabDataset
# data.create_dataset()
from functools import partial
partial_vtab = partial(VtabDataset, vtab_dir=config.vtab_dir, subset_name=config.subset_name,)
get_vtab_dataset = lambda split, batch_size=config.initial_batch_size : data.create_loader(partial_vtab(split=split), input_size=(3, 224, 224), batch_size=batch_size, num_workers=4)
def init_data(self, config):
    # return # data in format [(img:PIL, label:int)]
    self.train_dataset = get_vtab_dataset(VtabSplit.TRAIN)
    self.val_dataset = get_vtab_dataset(VtabSplit.VAL)
    self.train_val_dataset = get_vtab_dataset(VtabSplit.TRAIN_AND_VAL)
    self.test_dataset = get_vtab_dataset(VtabSplit.TEST)
    self.num_of_classes = len(self.train_dataset.dataset.classes)

# %% ../nbs/00_core.ipynb 12
import lightning as L
from overrides import override
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
class ClassificationTask(L.LightningModule):
    init_data = init_data
    init_head = init_head
    init_backbone = init_backbone
    

    def __init__(self, config: ClassificationTaskConfig)->None:
        super().__init__()
        self.config = config
        init_env(config.experiment_index) # use index as the seed for reproducibility
        # init_data(self, config)
        self.init_data(config)
        self.init_backbone(config)
        self.init_head(self.hidden_dim, self.num_of_classes)
        # https://blog.csdn.net/qq_43391414/article/details/118421352 logsoftmax+nll的速度快，但是没有label smoothing
        self.model = nn.Sequential(self.backbone, self.head, nn.LogSoftmax(dim=1))
        # self.model = nn.Sequential(self.backbone, self.head, nn.Softmax(dim=1))
        self.forward = self.model.forward
    # @override
    # def forward(self, x):
        # out = self.backbone(x)
        # out = self.head(out)
        # return F.log_softmax(out, dim=1)
        # return self.model(x)
    def forward_loss(self, image_tensor, label_tensor):
        logits = self(image_tensor)
        # return F.nll_loss(logits, label_tensor)
        return F.cross_entropy(logits, label_tensor, label_smoothing=0.1)
    # @override
        
    def training_step(self, batch, batch_idx):
        loss = self.forward_loss(*batch)
        self.log("train_loss", loss)
        return loss
    
    def evaluate(self, batch, stage=None):
        loss = self.forward_loss(*batch)

    # @override
    # def 
