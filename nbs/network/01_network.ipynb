{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp network\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"../pretrains/swin-base-patch4-window7-224-in22k-finetuned-cifar10\"\n",
    "from transformers import AutoImageProcessor\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6414c28ebbe5419b838cce26df783933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ec7a349b37425d851ff2f6d1d9045f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.dinov2.modeling_dinov2 import Dinov2Model\n",
    "# import inspect\n",
    "# print(inspect.getfile(Dinov2Model))\n",
    "from transformers import Dinov2Model, Dinov2Config\n",
    "model = Dinov2Model(Dinov2Config(\n",
    "    hidden_size=768//6,\n",
    "    num_attention_heads=12//6,\n",
    "    num_hidden_layers=12,\n",
    "    mlp_ratio=4,    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.dinov2.modeling_dinov2.Dinov2Model"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ai_pitch_perfector/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# dinov2_vits14_lc \n",
    "# dinov2_vits14 \n",
    "# dinov2_vits14_reg_lc\n",
    "dinov2_vits14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "model =    dinov2_vits14_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ai_pitch_perfector/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/ai_pitch_perfector/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "small_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "big_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 3, 7, 7]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64] bias:[64]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   └── <span style=\"color: #800000; text-decoration-color: #800000\">0-1</span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">conv1,conv2</span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 64, 3, 3]</span>\n",
       "│       └── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64] bias:[64]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 64, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128] bias:[128]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 128, 3, 3]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 64, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128] bias:[128]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">conv1,conv2</span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 128, 3, 3]</span>\n",
       "│       └── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128] bias:[128]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 128, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 256, 3, 3]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 128, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">conv1,conv2</span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 256, 3, 3]</span>\n",
       "│       └── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer4 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 256, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 512, 3, 3]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 256, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BasicBlock)</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">conv1,conv2</span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 512, 3, 3]</span>\n",
       "│       └── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fc </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1000, 512] bias:[1000]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 3, 7, 7]\u001b[0m\n",
       "├── \u001b[37mbn1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[64] \u001b[0m\u001b[36mbias:[64]\u001b[0m\n",
       "├── \u001b[37mlayer1 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   └── \u001b[31m0-1\u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│       ├── \u001b[31mconv1,conv2\u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 64, 3, 3]\u001b[0m\n",
       "│       └── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[64] \u001b[0m\u001b[36mbias:[64]\u001b[0m\n",
       "├── \u001b[37mlayer2 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 64, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[128] \u001b[0m\u001b[36mbias:[128]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 128, 3, 3]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 64, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[128] \u001b[0m\u001b[36mbias:[128]\u001b[0m\n",
       "│   └── \u001b[37m1 \u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│       ├── \u001b[31mconv1,conv2\u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 128, 3, 3]\u001b[0m\n",
       "│       └── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[128] \u001b[0m\u001b[36mbias:[128]\u001b[0m\n",
       "├── \u001b[37mlayer3 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 128, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 256, 3, 3]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 128, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "│   └── \u001b[37m1 \u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│       ├── \u001b[31mconv1,conv2\u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 256, 3, 3]\u001b[0m\n",
       "│       └── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "├── \u001b[37mlayer4 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 256, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 512, 3, 3]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 256, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "│   └── \u001b[37m1 \u001b[0m\u001b[32m(BasicBlock)\u001b[0m\n",
       "│       ├── \u001b[31mconv1,conv2\u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 512, 3, 3]\u001b[0m\n",
       "│       └── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "└── \u001b[37mfc \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1000, 512] \u001b[0m\u001b[36mbias:[1000]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 3, 7, 7]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64] bias:[64]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 64, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64] bias:[64]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 64, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 64, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 64, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "│   └── <span style=\"color: #800000; text-decoration-color: #800000\">1-2</span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 256, 1, 1]</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64] bias:[64]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[64, 64, 3, 3]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 64, 1, 1]</span>\n",
       "│       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 256, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128] bias:[128]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 128, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 128, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 256, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "│   └── <span style=\"color: #800000; text-decoration-color: #800000\">1-7</span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 512, 1, 1]</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128] bias:[128]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[128, 128, 3, 3]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 128, 1, 1]</span>\n",
       "│       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 512, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 256, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1024, 256, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1024] bias:[1024]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1024, 512, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1024] bias:[1024]</span>\n",
       "│   └── <span style=\"color: #800000; text-decoration-color: #800000\">1-35</span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 1024, 1, 1]</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256] bias:[256]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[256, 256, 3, 3]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1024, 256, 1, 1]</span>\n",
       "│       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1024] bias:[1024]</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layer4 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 1024, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 512, 3, 3]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[2048, 512, 1, 1]</span>\n",
       "│   │   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[2048] bias:[2048]</span>\n",
       "│   │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">downsample </span><span style=\"color: #008000; text-decoration-color: #008000\">(Sequential)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[2048, 1024, 1, 1]</span>\n",
       "│   │       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[2048] bias:[2048]</span>\n",
       "│   └── <span style=\"color: #800000; text-decoration-color: #800000\">1-2</span><span style=\"color: #008000; text-decoration-color: #008000\">(Bottleneck)</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv1 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 2048, 1, 1]</span>\n",
       "│       ├── <span style=\"color: #800000; text-decoration-color: #800000\">bn1,bn2</span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512] bias:[512]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv2 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[512, 512, 3, 3]</span>\n",
       "│       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">conv3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(Conv2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[2048, 512, 1, 1]</span>\n",
       "│       └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">bn3 </span><span style=\"color: #008000; text-decoration-color: #008000\">(BatchNorm2d) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[2048] bias:[2048]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">fc </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[1000, 2048] bias:[1000]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 3, 7, 7]\u001b[0m\n",
       "├── \u001b[37mbn1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[64] \u001b[0m\u001b[36mbias:[64]\u001b[0m\n",
       "├── \u001b[37mlayer1 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 64, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[64] \u001b[0m\u001b[36mbias:[64]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 64, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 64, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 64, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "│   └── \u001b[31m1-2\u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│       ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 256, 1, 1]\u001b[0m\n",
       "│       ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[64] \u001b[0m\u001b[36mbias:[64]\u001b[0m\n",
       "│       ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[64, 64, 3, 3]\u001b[0m\n",
       "│       ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 64, 1, 1]\u001b[0m\n",
       "│       └── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "├── \u001b[37mlayer2 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 256, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[128] \u001b[0m\u001b[36mbias:[128]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 128, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 128, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 256, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "│   └── \u001b[31m1-7\u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│       ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 512, 1, 1]\u001b[0m\n",
       "│       ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[128] \u001b[0m\u001b[36mbias:[128]\u001b[0m\n",
       "│       ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[128, 128, 3, 3]\u001b[0m\n",
       "│       ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 128, 1, 1]\u001b[0m\n",
       "│       └── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "├── \u001b[37mlayer3 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 512, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 256, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[1024, 256, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[1024] \u001b[0m\u001b[36mbias:[1024]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[1024, 512, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[1024] \u001b[0m\u001b[36mbias:[1024]\u001b[0m\n",
       "│   └── \u001b[31m1-35\u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│       ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 1024, 1, 1]\u001b[0m\n",
       "│       ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[256] \u001b[0m\u001b[36mbias:[256]\u001b[0m\n",
       "│       ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[256, 256, 3, 3]\u001b[0m\n",
       "│       ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[1024, 256, 1, 1]\u001b[0m\n",
       "│       └── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[1024] \u001b[0m\u001b[36mbias:[1024]\u001b[0m\n",
       "├── \u001b[37mlayer4 \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   ├── \u001b[37m0 \u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│   │   ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 1024, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 512, 3, 3]\u001b[0m\n",
       "│   │   ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[2048, 512, 1, 1]\u001b[0m\n",
       "│   │   ├── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[2048] \u001b[0m\u001b[36mbias:[2048]\u001b[0m\n",
       "│   │   └── \u001b[37mdownsample \u001b[0m\u001b[32m(Sequential)\u001b[0m\n",
       "│   │       ├── \u001b[37m0 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[2048, 1024, 1, 1]\u001b[0m\n",
       "│   │       └── \u001b[37m1 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[2048] \u001b[0m\u001b[36mbias:[2048]\u001b[0m\n",
       "│   └── \u001b[31m1-2\u001b[0m\u001b[32m(Bottleneck)\u001b[0m\n",
       "│       ├── \u001b[37mconv1 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 2048, 1, 1]\u001b[0m\n",
       "│       ├── \u001b[31mbn1,bn2\u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[512] \u001b[0m\u001b[36mbias:[512]\u001b[0m\n",
       "│       ├── \u001b[37mconv2 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[512, 512, 3, 3]\u001b[0m\n",
       "│       ├── \u001b[37mconv3 \u001b[0m\u001b[32m(Conv2d) \u001b[0m\u001b[36mweight:[2048, 512, 1, 1]\u001b[0m\n",
       "│       └── \u001b[37mbn3 \u001b[0m\u001b[32m(BatchNorm2d) \u001b[0m\u001b[36mweight:[2048] \u001b[0m\u001b[36mbias:[2048]\u001b[0m\n",
       "└── \u001b[37mfc \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[1000, 2048] \u001b[0m\u001b[36mbias:[1000]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bigmodelvis import Visualization\n",
    "Visualization(small_model).structure_graph()\n",
    "Visualization(big_model).structure_graph()\n",
    "pass\n",
    "model = big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_data = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dinov2\n",
    "dinov2.models.vision_transformer\n",
    "from dinov2.models.vision_transformer import DinoVisionTransformer, partial, Block, MemEffAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2702, -1.3297, -1.8049,  0.9489, -0.3901, -0.9309,  0.2981, -0.8007,\n",
       "         -1.3131, -0.5862,  1.6103, -1.3929, -0.2652, -0.6405,  0.3781,  0.2629,\n",
       "          0.4109, -2.4554,  0.1292, -1.2208, -0.4040,  1.2774,  0.6358, -0.6957,\n",
       "         -0.8603,  2.1823, -1.4509,  0.3983,  1.7402,  0.0130, -0.8846,  0.3883,\n",
       "          1.2104, -0.6616, -1.1842, -0.6157,  1.5368,  1.2777,  0.3609,  0.4091,\n",
       "          1.4440,  1.1258, -2.1607, -0.8560, -1.1688, -1.1146, -0.1817,  1.3788,\n",
       "         -0.2979, -1.7878,  0.3192,  0.9526,  0.0536, -0.4929,  0.5445, -1.1738,\n",
       "          0.2333, -0.3452, -0.7731,  1.0652, -0.0303,  0.0045, -0.3780,  1.1885,\n",
       "         -0.2599,  0.0368,  0.5167,  1.5816,  0.5415,  0.0673, -0.7279,  0.4604,\n",
       "         -0.1261,  1.2731,  0.5058,  0.7985,  0.5639,  0.7272,  0.3229,  0.0676,\n",
       "         -0.6536,  1.0942, -1.9395,  0.4661, -0.2648, -0.7735,  0.9986, -1.3300,\n",
       "          0.8795,  1.6248,  0.8188, -1.1171,  0.6380,  0.9743, -1.5981,  0.8639,\n",
       "         -0.3361, -1.1781,  0.8800, -0.3170,  0.2050, -0.3005, -1.9083,  0.6991,\n",
       "          0.1375,  1.3266,  0.5651, -0.4488, -1.3917, -1.0405,  2.3734, -0.2646,\n",
       "          1.0519, -0.3682, -1.3046, -0.9521,  0.2356,  0.4831,  0.3190, -0.2380,\n",
       "          0.2306, -1.3439,  1.7966, -0.5078, -0.1647,  2.3188, -0.1036, -0.8856]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=MemEffAttention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "def vit_dwarf(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=384//3,\n",
    "        depth=12,\n",
    "        num_heads=6//3,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=MemEffAttention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "model = vit_dwarf()\n",
    "model.cuda()(test_data.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigmodelvis import Visualization\n",
    "Visualization(model).structure_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/zugexiaodui/torch_flops\n",
    "# https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mNOTE: The model cannot be built as a graph model by 'symbolic_trace()'. Please remove the `assert`, `if` and `for` operations. See 'https://pytorch.org/docs/stable/fx.html#limitations-of-symbolic-tracing' for more instructions.\u001b[0m\n"
     ]
    },
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()(test_data\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_flops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchFLOPsByFX\n\u001b[0;32m----> 4\u001b[0m flops_counter \u001b[38;5;241m=\u001b[39m \u001b[43mTorchFLOPsByFX\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/torch_flops/flops_engine.py:387\u001b[0m, in \u001b[0;36mTorchFLOPsByFX.__init__\u001b[0;34m(self, model, mem_func_name, ignore_ops)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mTraceError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[33mNOTE: The model cannot be built as a graph model by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolic_trace()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please remove the `assert`, `if` and `for` operations. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    386\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/stable/fx.html#limitations-of-symbolic-tracing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for more instructions.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[33mNOTE: The model cannot be built as a graph model by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolic_trace()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please replace the `tensor.shape[i]` that servers as the parameter of a function with a pre-defined deterministic value.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/torch_flops/flops_engine.py:383\u001b[0m, in \u001b[0;36mTorchFLOPsByFX.__init__\u001b[0;34m(self, model, mem_func_name, ignore_ops)\u001b[0m\n\u001b[1;32m    381\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_model: GraphModule \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mTraceError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[33mNOTE: The model cannot be built as a graph model by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolic_trace()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please remove the `assert`, `if` and `for` operations. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    386\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/stable/fx.html#limitations-of-symbolic-tracing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for more instructions.\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1222\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1222\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1224\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1225\u001b[0m )\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _make_graph_module(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:822\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    816\u001b[0m             _autowrap_check(\n\u001b[1;32m    817\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    818\u001b[0m             )\n\u001b[1;32m    819\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    820\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 822\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    823\u001b[0m             {},\n\u001b[1;32m    824\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/transformers/models/dinov2/modeling_dinov2.py:628\u001b[0m, in \u001b[0;36mDinov2Model.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_head_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[1;32m    632\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    633\u001b[0m     embedding_output,\n\u001b[1;32m    634\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    638\u001b[0m )\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1151\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_head_mask\u001b[0;34m(self, head_mask, num_hidden_layers, is_attention_chunked)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03mPrepare the head mask if needed.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    `[None]` for each layer.\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1151\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_head_mask_to_5d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_attention_chunked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m         head_mask \u001b[38;5;241m=\u001b[39m head_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1161\u001b[0m, in \u001b[0;36mModuleUtilsMixin._convert_head_mask_to_5d\u001b[0;34m(self, head_mask, num_hidden_layers)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_head_mask_to_5d\u001b[39m(\u001b[38;5;28mself\u001b[39m, head_mask, num_hidden_layers):\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"-> [num_hidden_layers x batch x num_heads x seq_length x seq_length]\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m head_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1162\u001b[0m         head_mask \u001b[38;5;241m=\u001b[39m head_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1163\u001b[0m         head_mask \u001b[38;5;241m=\u001b[39m head_mask\u001b[38;5;241m.\u001b[39mexpand(num_hidden_layers, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/torch/fx/proxy.py:447\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/program_files/managers/conda/lib/python3.10/site-packages/torch/fx/proxy.py:307\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.cuda()(test_data.cuda())\n",
    "from torch_flops import TorchFLOPsByFX\n",
    "flops_counter = TorchFLOPsByFX(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai_pitch_perfector/program_files/managers/conda/lib/python3.10/site-packages/fvcore/nn/activation_count.py\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, ActivationCountAnalysis, flop_count_table, activation_count\n",
    "import inspect\n",
    "print(inspect.getfile(ActivationCountAnalysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                                           | #parameters or shape   | #flops     |\n",
      "|:-------------------------------------------------|:-----------------------|:-----------|\n",
      "| model                                            | 2.506M                 | 0.606G     |\n",
      "|  embeddings                                      |  0.124M                |  19.268M   |\n",
      "|   embeddings.cls_token                           |   (1, 1, 128)          |            |\n",
      "|   embeddings.mask_token                          |   (1, 128)             |            |\n",
      "|   embeddings.position_embeddings                 |   (1, 197, 128)        |            |\n",
      "|   embeddings.patch_embeddings.projection         |   98.432K              |   19.268M  |\n",
      "|    embeddings.patch_embeddings.projection.weight |    (128, 3, 16, 16)    |            |\n",
      "|    embeddings.patch_embeddings.projection.bias   |    (128,)              |            |\n",
      "|  encoder.layer                                   |  2.382M                |  0.587G    |\n",
      "|   encoder.layer.0                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.0.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.0.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.0.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.0.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.0.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.0.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.1                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.1.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.1.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.1.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.1.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.1.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.1.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.2                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.2.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.2.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.2.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.2.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.2.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.2.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.3                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.3.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.3.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.3.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.3.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.3.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.3.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.4                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.4.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.4.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.4.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.4.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.4.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.4.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.5                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.5.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.5.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.5.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.5.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.5.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.5.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.6                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.6.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.6.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.6.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.6.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.6.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.6.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.7                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.7.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.7.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.7.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.7.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.7.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.7.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.8                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.8.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.8.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.8.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.8.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.8.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.8.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.9                                |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.9.norm1                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.9.attention                     |    66.048K             |    22.846M |\n",
      "|    encoder.layer.9.layer_scale1                  |    0.128K              |    0       |\n",
      "|    encoder.layer.9.norm2                         |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.9.mlp                           |    0.132M              |    25.821M |\n",
      "|    encoder.layer.9.layer_scale2                  |    0.128K              |    0       |\n",
      "|   encoder.layer.10                               |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.10.norm1                        |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.10.attention                    |    66.048K             |    22.846M |\n",
      "|    encoder.layer.10.layer_scale1                 |    0.128K              |    0       |\n",
      "|    encoder.layer.10.norm2                        |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.10.mlp                          |    0.132M              |    25.821M |\n",
      "|    encoder.layer.10.layer_scale2                 |    0.128K              |    0       |\n",
      "|   encoder.layer.11                               |   0.199M               |   48.919M  |\n",
      "|    encoder.layer.11.norm1                        |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.11.attention                    |    66.048K             |    22.846M |\n",
      "|    encoder.layer.11.layer_scale1                 |    0.128K              |    0       |\n",
      "|    encoder.layer.11.norm2                        |    0.256K              |    0.126M  |\n",
      "|    encoder.layer.11.mlp                          |    0.132M              |    25.821M |\n",
      "|    encoder.layer.11.layer_scale2                 |    0.128K              |    0       |\n",
      "|  layernorm                                       |  0.256K                |  0.126M    |\n",
      "|   layernorm.weight                               |   (128,)               |            |\n",
      "|   layernorm.bias                                 |   (128,)               |            |\n",
      "| module                                           | #parameters or shape   | #flops    |\n",
      "|:-------------------------------------------------|:-----------------------|:----------|\n",
      "| model                                            | 2.506M                 | 3.982M    |\n",
      "|  embeddings                                      |  0.124M                |  25.088K  |\n",
      "|   embeddings.cls_token                           |   (1, 1, 128)          |           |\n",
      "|   embeddings.mask_token                          |   (1, 128)             |           |\n",
      "|   embeddings.position_embeddings                 |   (1, 197, 128)        |           |\n",
      "|   embeddings.patch_embeddings.projection         |   98.432K              |   25.088K |\n",
      "|    embeddings.patch_embeddings.projection.weight |    (128, 3, 16, 16)    |           |\n",
      "|    embeddings.patch_embeddings.projection.bias   |    (128,)              |           |\n",
      "|  encoder.layer                                   |  2.382M                |  3.957M   |\n",
      "|   encoder.layer.0                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.0.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.0.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.0.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.0.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.0.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.0.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.1                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.1.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.1.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.1.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.1.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.1.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.1.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.2                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.2.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.2.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.2.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.2.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.2.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.2.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.3                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.3.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.3.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.3.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.3.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.3.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.3.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.4                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.4.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.4.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.4.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.4.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.4.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.4.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.5                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.5.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.5.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.5.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.5.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.5.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.5.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.6                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.6.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.6.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.6.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.6.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.6.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.6.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.7                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.7.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.7.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.7.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.7.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.7.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.7.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.8                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.8.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.8.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.8.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.8.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.8.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.8.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.9                                |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.9.norm1                         |    0.256K              |    0      |\n",
      "|    encoder.layer.9.attention                     |    66.048K             |    0.204M |\n",
      "|    encoder.layer.9.layer_scale1                  |    0.128K              |    0      |\n",
      "|    encoder.layer.9.norm2                         |    0.256K              |    0      |\n",
      "|    encoder.layer.9.mlp                           |    0.132M              |    0.126M |\n",
      "|    encoder.layer.9.layer_scale2                  |    0.128K              |    0      |\n",
      "|   encoder.layer.10                               |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.10.norm1                        |    0.256K              |    0      |\n",
      "|    encoder.layer.10.attention                    |    66.048K             |    0.204M |\n",
      "|    encoder.layer.10.layer_scale1                 |    0.128K              |    0      |\n",
      "|    encoder.layer.10.norm2                        |    0.256K              |    0      |\n",
      "|    encoder.layer.10.mlp                          |    0.132M              |    0.126M |\n",
      "|    encoder.layer.10.layer_scale2                 |    0.128K              |    0      |\n",
      "|   encoder.layer.11                               |   0.199M               |   0.33M   |\n",
      "|    encoder.layer.11.norm1                        |    0.256K              |    0      |\n",
      "|    encoder.layer.11.attention                    |    66.048K             |    0.204M |\n",
      "|    encoder.layer.11.layer_scale1                 |    0.128K              |    0      |\n",
      "|    encoder.layer.11.norm2                        |    0.256K              |    0      |\n",
      "|    encoder.layer.11.mlp                          |    0.132M              |    0.126M |\n",
      "|    encoder.layer.11.layer_scale2                 |    0.128K              |    0      |\n",
      "|  layernorm                                       |  0.256K                |  0        |\n",
      "|   layernorm.weight                               |   (128,)               |           |\n",
      "|   layernorm.bias                                 |   (128,)               |           |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fvcore.nn import FlopCountAnalysis, ActivationCountAnalysis, flop_count_table, activation_count\n",
    "# test_data = torch.randn(10, 3, 224, 224)\n",
    "test_data = torch.randn(1, 3, 224, 224) # 复现 resnet 论文的输入， random crop出224 224\n",
    "flops = FlopCountAnalysis(model.cuda(), test_data.cuda())\n",
    "# # flops.total()\n",
    "# # flops.by_operator()\n",
    "# # flops.by_module()\n",
    "# # flops.by_module_and_operator()\n",
    "print(flop_count_table(flops))\n",
    "act = ActivationCountAnalysis(model.cuda(), test_data.cuda())\n",
    "print(flop_count_table(act))\n",
    "# 86.58M\n",
    "# k M G 都是1000为单位\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3982424"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
