{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Training and evaluating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "# 数据集决定\n",
    "# - 类别数量\n",
    "# - 最佳增强方式\n",
    "# 决定预处理和模型的部分结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ClassificationTaskConfig(BaseModel):\n",
    "    vtab_dir: str = \"/home/ai_pitch_perfector/datasets/vtab-1k/\"\n",
    "    subset_name: str = \"cifar\"\n",
    "    initial_batch_size: int = 64\n",
    "    experiment_index: int = 0  # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 表示是第几次重复实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ClassificationTaskConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "def init_env(seed: int = 42):\n",
    "    # Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "    L.seed_everything(seed)\n",
    "    import torch\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "from sota_image_classification import data\n",
    "from sota_image_classification.data.vtab import VtabSplit, VtabDataset\n",
    "\n",
    "# data.create_dataset()\n",
    "from functools import partial\n",
    "\n",
    "partial_vtab = partial(\n",
    "    VtabDataset,\n",
    "    vtab_dir=config.vtab_dir,\n",
    "    subset_name=config.subset_name,\n",
    ")\n",
    "get_vtab_dataset = (\n",
    "    lambda split, batch_size=config.initial_batch_size: data.create_loader(\n",
    "        partial_vtab(split=split),\n",
    "        input_size=(3, 224, 224),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def init_data(self, config):\n",
    "    # return # data in format [(img:PIL, label:int)]\n",
    "    self.train_dataset = get_vtab_dataset(VtabSplit.TRAIN)\n",
    "    self.val_dataset = get_vtab_dataset(VtabSplit.VAL)\n",
    "    self.train_val_dataset = get_vtab_dataset(VtabSplit.TRAIN_AND_VAL)\n",
    "    self.test_dataset = get_vtab_dataset(VtabSplit.TEST)\n",
    "    self.num_of_classes = len(self.train_dataset.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = lambda: None\n",
    "init_data(self, config)\n",
    "train_dataset = self.train_dataset\n",
    "next(iter(train_dataset))\n",
    "train_dataset.dataset[0]\n",
    "# train_dataset.dataset.classes\n",
    "num_of_classes = len(train_dataset.dataset.classes)\n",
    "num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_backbone(self, config):\n",
    "    from transformers import Dinov2Model, Dinov2Config\n",
    "\n",
    "    self.backbone = Dinov2Model(\n",
    "        Dinov2Config(\n",
    "            hidden_size=768 // 6,\n",
    "            num_attention_heads=12 // 6,\n",
    "            num_hidden_layers=12,\n",
    "            mlp_ratio=4,\n",
    "        )\n",
    "    )\n",
    "    # self.backbone.forward = partial(self.backbone.forward,\n",
    "    #                                 # output_hidden_states=True,\n",
    "    #                                 # return_dict=False\n",
    "    #                                 )\n",
    "    old_forward = self.backbone.forward\n",
    "    self.backbone.forward = lambda x: old_forward(x).pooler_output\n",
    "\n",
    "    self.hidden_dim = 768 // 6\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def init_head(self, hidden_dim: int, num_of_classes: int):\n",
    "    self.head = nn.Linear(in_features=hidden_dim, out_features=num_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = lambda: None\n",
    "init_backbone(self, config)\n",
    "# try_data = train_dataset.dataset[0][0]\n",
    "try_data = next(iter(train_dataset))[0]\n",
    "# try_data[0].shape\n",
    "# self.backbone.cuda()(try_data).last_hidden_state.shape\n",
    "# self.backbone.cuda()(try_data).pooler_output.shape\n",
    "self.backbone.cuda()(try_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_array(x:torch.Tensor):\n",
    "    return x.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "# from scipy.special import softmax\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def ensure_array(x:torch.Tensor):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def compute_classification_metrics(\n",
    "    y_true: np.ndarray,  # 1d array-like, or label indicator array / sparse matrix\n",
    "    y_pred_logits: np.ndarray,  # label indicator array / sparse matrix\n",
    "    logits_to_prob: bool = False,  # function to convert logits to probabilities\n",
    "):\n",
    "    \n",
    "    # print(type(y_pred_logits)) # <class 'numpy.ndarray'>\n",
    "    # y_pred_probs = softmax(y_pred_logits)# label indicator array / sparse matrix\n",
    "    y_pred_probs = (\n",
    "        np.array(F.softmax(torch.Tensor(y_pred_logits), dim=1))\n",
    "        if logits_to_prob\n",
    "        else y_pred_logits\n",
    "    )  # label indicator array / sparse matrix\n",
    "    y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "    # target_names = labels # dataset['train'].features[label_column_name].names\n",
    "    # report_dict = classification_report(y_true, y_pred_probs, target_names=target_names, output_dict=True)\n",
    "    top_k_res = {\n",
    "        f\"acc{k}\": top_k_accuracy_score(y_true, y_pred_probs, k=k)\n",
    "        for k in [1, 2, 3, 5, 10, 20]\n",
    "    }\n",
    "    balance_res = dict(\n",
    "        roc_auc=roc_auc_score(\n",
    "            y_true, y_pred_probs, average=\"macro\", multi_class=\"ovr\"\n",
    "        ),  # ovr更难一些，会不平衡\n",
    "        matthews_corrcoef=matthews_corrcoef(y_true, y_pred),\n",
    "        f1=f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        precision=precision_score(y_true, y_pred, average=\"macro\"),\n",
    "        recall=recall_score(y_true, y_pred, average=\"macro\"),\n",
    "        log_loss=log_loss(\n",
    "            y_true,\n",
    "            y_pred_probs,\n",
    "        ),\n",
    "        balanced_accuracy=balanced_accuracy_score(y_true, y_pred),\n",
    "        cohen_kappa=cohen_kappa_score(y_true, y_pred),\n",
    "        hinge_loss=hinge_loss(y_true, y_pred_probs, labels=ids),\n",
    "    )\n",
    "\n",
    "    # return top_k_res| balance_res| report_dict\n",
    "    return top_k_res | balance_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import lightning as L\n",
    "from overrides import override\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "class ClassificationTask(L.LightningModule):\n",
    "    init_data = init_data\n",
    "    init_head = init_head\n",
    "    init_backbone = init_backbone\n",
    "    \n",
    "\n",
    "    def __init__(self, config: ClassificationTaskConfig)->None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        init_env(config.experiment_index) # use index as the seed for reproducibility\n",
    "        # init_data(self, config)\n",
    "        self.init_data(config)\n",
    "        self.init_backbone(config)\n",
    "        self.init_head(self.hidden_dim, self.num_of_classes)\n",
    "        # https://blog.csdn.net/qq_43391414/article/details/118421352 logsoftmax+nll的速度快，但是没有label smoothing\n",
    "        self.model = nn.Sequential(self.backbone, self.head, nn.LogSoftmax(dim=1))\n",
    "        # self.model = nn.Sequential(self.backbone, self.head, nn.Softmax(dim=1))\n",
    "        self.forward = self.model.forward\n",
    "    # @override\n",
    "    # def forward(self, x):\n",
    "        # out = self.backbone(x)\n",
    "        # out = self.head(out)\n",
    "        # return F.log_softmax(out, dim=1)\n",
    "        # return self.model(x)\n",
    "    def forward_loss(self, image_tensor, label_tensor):\n",
    "        logits = self(image_tensor)\n",
    "        # return F.nll_loss(logits, label_tensor)\n",
    "        return F.cross_entropy(logits, label_tensor, label_smoothing=0.1)\n",
    "    # @override\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward_loss(*batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, batch, stage=None):\n",
    "        loss = self.forward_loss(*batch)\n",
    "\n",
    "    # @override\n",
    "    # def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = ClassificationTask(config)\n",
    "task.cuda()(try_data).shape\n",
    "# import inspect\n",
    "# inspect.getsource(task.init_data)\n",
    "# task.init_data\n",
    "# task.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
